# [プロジェクト名：DQNによるBreakout攻略]

## 概要
このリポジトリは、深層強化学習(DQN)を用いてAtariのゲーム「Breakout」を攻略するエージェントのプロジェクトです。

## 背景と目的
第２回講義「DQNでゲームを攻略するエージェントを学習」で扱われた「Pong」よりも複雑な「Breakout」を対象とし、DQNエージェントの性能と、より困難なタスクを攻略する過程で発生する課題を検証することを目的としました。

## アプローチと手法
- **アルゴリズム:** DQN (Deep Q-Network)
- **ネットワークアーキテクチャ:** Dueling Network
- **経験再生:** Prioritized Experience Replay (PER)
- **状態表現:** FrameStack (4フレームを1つの状態として使用)

## 実験プロセスと課題解決
本プロジェクトは、単一の学習で完結せず、発生した課題を分析し、解決策を講じる反復的なプロセスを経て進められました。

1.  **環境構築:** 最新ライブラリである`gymnasium`へ移行し、APIの仕様変更に対応しました。
2.  **課題①：学習の不安定性:** 10000エピソードの連続学習時に、メモリリークによるクラッシュが頻発しました。
3.  **解決策①：チェックポイント戦略:** 学習を2000エピソードのチャンクに分割し、進捗を逐次保存することで、安定した学習環境を構築しました。
4.  **課題②：局所最適解への陥り:** しかし、上記の手法ではリプレイバッファが毎回リセットされる副作用により、エージェントが「無限ラリー」のような異常行動を学習してしまう問題が発生しました。
5.  **解決策②：ハイパーパラメータ調整:** この停滞を打開するため、学習率と探索期間をより攻撃的な設定に変更し、追加実験を行いました。その結果、エージェントは局所最適解から脱出し、最終的にスコアを向上させる戦略の獲得に成功しました。

## 使用技術
- Python
- PyTorch
- Gymnasium
- Matplotlib
- Pandas
- TensorBoard

## 実行方法
`[DQN_Breakout_Project.ipynb]` をJupyter NotebookまたはGoogle Colab環境で開き、上から順番にセルを実行してください。

## プロジェクト詳細
より詳しいレポートや考察については、以下のCosenseページをご覧ください。

https://scrapbox.io/rl2025/DQN%E3%81%AB%E3%82%88%E3%82%8BBreakout%E6%94%BB%E7%95%A5
